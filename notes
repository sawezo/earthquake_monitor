## FLOWCHART
![data architecture flowchart](../docs/imgs/flowchart.png "Data Architecture Flowchart")


#### Overview of program 

### Overview of Technology Stack



## Post
## KAFKA
Apache Kafka is a distributed streaming platform that:

Lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system.
Lets you store streams of records in a fault-tolerant way.
Lets you process streams of records as they occur.

Every message that is feed into the system must be part of some topic. The topic is nothing but a stream of records. The messages are stored in key-value format. Each message is assigned a sequence, called Offset. The output of one message could be an input of the other for further processing.
Producers
Producers are the apps responsible to publish data into Kafka system. They publish data on the topic of their choice.
Consumers
The messages published into topics are then utilized by Consumers apps. A consumer gets subscribed to the topic of its choice and consumes data.

Broker
Every instance of Kafka that is responsible for message exchange is called a Broker. Kafka can be used as a stand-alone machine or a part of a cluster.
I try to explain the whole thing with a simple example, there is a warehouse or godown of a restaurant where all the raw material is dumped like rice, vegetables etc. The restaurant serves different kinds of dishes: Chinese, Desi, Italian etc. The chefs of each cuisine can refer to the warehouse, pick the desire things and make things. There is a possibility that the stuff made by the raw material can later be used by all departments’ chefs, for instance, some secret sauce that is used in ALL kind of dishes. Here, the warehouse is a broker, vendors of goods are the producers, the goods and the secret sauce made by chefs are topics while chefs are consumers. 


## FLINK 
Note that Flink's metrics only report bytes and records and records communicated within the Flink cluster, and so will always report 0 bytes and 0 records received by sources, and 0 bytes and 0 records sent to sinks - so don't be confused that noting is reported as being read from Kafka, or written to Elasticsearch.


## KAPPA

https://eng.uber.com/kappa-architecture-data-stream-processing/

A streaming architecture is a defined set of technologies that work together to handle [stream processing](https://hazelcast.com/glossary/stream-processing/), which is the practice of taking action on a series of data at the time the data is created. In many modern deployments, Apache Kafka acts as the store for the streaming data, and then multiple stream processors can act on the data stored in Kafka to produce multiple outputs. Some streaming architectures include workflows for both stream processing and [batch processing](https://hazelcast.com/glossary/micro-batch-processing/), which either entails other technologies to handle large-scale batch processing, or using Kafka as the central store as specified in the Kappa Architecture.

Leveraging a Lambda architecture allows engineers to reliably backfill a streaming pipeline*,* but it also requires maintaining two disparate codebases, one for batch and one for streaming. While the streaming pipeline runs in real time, the batch pipeline is scheduled at a delayed interval to reprocess data for the most accurate results. While a Lambda architecture provides many benefits, it also introduces the difficulty of having to reconcile business logic across streaming and batch codebases. 

To counteract these limitations, Apache Kafka’s co-creator [Jay Kreps suggested using a Kappa architecture](https://www.oreilly.com/ideas/questioning-the-lambda-architecture) for stream processing systems**.** Kreps’ key idea was to replay data into a Kafka stream from a structured data source such as an Apache Hive table. This setup then simply reruns the streaming job on these replayed Kafka topics, achieving a unified codebase between both batch and streaming pipelines and production and backfill use cases.



## model
Running the example prints the input and output portions of the samples with the output values for the next time step rather than the current time step as we may desire for this type of problem.
https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/

For a stateful LSTM, the batch size should be chosen in a way, so that the number of samples is divisible by the batch size. See also here:

Keras: What if the size of data is not divisible by batch_size?

In your case, considering that you take 20% from your training data as a validation set, you have 1136 samples remaining. So you should choose a batch size by which 1136 is divisible.

Additionally, you could for example remove some samples or reuse samples to be able to choose various batch sizes.

I do not use validation sets for time series forecasting, I don’t see a straightforward way to implement it.

this is a multivariate, 1-step problem?

One big difference between regular regression models and time series models is how we run predictions. The first one should be pretty obvious, we take the last 12 months of train data and predict it to get the first test data.
How do we predict the next one?
This is a big issue esp. if you take a shortcut and use the test data’s first value and use that as your last prediction. That way you are feeding the correct values for the prior steps helping the model to create better results that it would otherwise give.
What needs to happen is that the “first prediction” needs to be added to the last 11 training data to create a new set of 12 data points to predict the next one. This way we are not cheating at all, the test data is really test data and is never seen by the model.

this post wasn't worth saving tho...

Assuming that the algorithms have a hard time to interpret GPS coordinates, the system
could instead use encoded positions. This approach would take a latitude and longitude position and with a defined precision hash it and save it in a dictionary. This precision would
be the amount of decimals for the latitude and longitude to use. This would discretize the
operating area into a grid where the interval depends on the chosen precision.
The drawback of this approach is with higher precision and operating area, the exponentially more locations have to hashed and stored. A small area of radius ten kilometre and
4 decimal precision could yield well over 300000 locations and therefore outcomes. Another
drawback is that the algorithm can only use positions it have trained on.


need to 1-hjot encode since no ordinal relation exists


## TODO
- make sure you update build.sh so that it doesn't break without env vars (test full project on other systems...)